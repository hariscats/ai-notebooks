{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a2b7aa",
   "metadata": {},
   "source": [
    "# Tokenization & Token Counting for Transformer NLP\n",
    "**Mission:** Build intuition for how raw text becomes token IDs, how counts affect attention cost, and how to reason about efficiency + limits. Each concept: explanation then a minimal code probe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bca519",
   "metadata": {},
   "source": [
    "## 1. What is Tokenization (and why models need discrete units)\n",
    "Transformers only operate on integer token IDs; these index embedding rows that become vectors the model can mix via attention. Tokenization bridges *text* â†’ *IDs* â†’ *embeddings* â†’ *attention matrices*. Without stable, consistent token boundaries, similarity, caching, cost, and context limits become unpredictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceef102",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = 'Transformers compress meaning.'\n",
    "print('Raw text:', sample_text)\n",
    "print('Characters:', len(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b316378",
   "metadata": {},
   "source": [
    "## 2. Raw Text â†’ Simple Word Tokens\n",
    "NaÃ¯ve split-based tokenization: fast but brittle (punctuation glued on, case differences create separate tokens, OOV not handled). Shows the *baseline* from which better methods improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f5e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sample_text.split()\n",
    "print('Word tokens:', words)\n",
    "print('Token count (word-level naive):', len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d59b19",
   "metadata": {},
   "source": [
    "## 3. Character-Level Tokenization (contrast)\n",
    "Character tokenization guarantees coverage (no OOV) but inflates length â†’ quadratic attention cost skyrockets; also weak semantic grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4f3398",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(sample_text)\n",
    "print('First 20 char tokens:', chars[:20])\n",
    "print('Total char tokens:', len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57bae1",
   "metadata": {},
   "source": [
    "## 4. Subword Tokenization Motivation\n",
    "Subword schemes (BPE / WordPiece) balance vocab size with ability to compose rare words. Idea: start from characters, iteratively merge frequent pairs to form a compact vocabulary that still decomposes unknown words. This reduces OOV while controlling sequence length vs. character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy illustration: naive pair frequency count (not a full BPE implementation).\n",
    "from collections import Counter\n",
    "word = 'compression'\n",
    "pairs = [word[i:i+2] for i in range(len(word)-1)]\n",
    "print('Word:', word)\n",
    "print('Adjacent bigrams:', pairs)\n",
    "print('Frequency:', Counter(pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d848d9",
   "metadata": {},
   "source": [
    "## 5. Using a Pretrained Tokenizer\n",
    "We load a real WordPiece tokenizer to see how it segments text into subwords and maps them to IDs (embedding indices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85d3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print('Model:', MODEL_NAME)\n",
    "print('Subword tokens:', tokens)\n",
    "print('Token IDs:', ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65ab6a",
   "metadata": {},
   "source": [
    "## 6. Special Tokens & IDs\n",
    "Special tokens add structure (classification heads, separation, padding, masking). They consume context length and affect counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b553c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Special tokens map:')\n",
    "for k,v in tokenizer.special_tokens_map.items():\n",
    "    print(f'  {k}: {v}')\n",
    "print('All special token IDs:', tokenizer.all_special_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa13ec4",
   "metadata": {},
   "source": [
    "## 7. Encoding to Model Inputs\n",
    "Encoding adds special tokens and produces attention masks (1=keep, 0=ignore). These arrays are the direct model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e45c948",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(sample_text, return_tensors='pt')\n",
    "print('input_ids shape:', encoded['input_ids'].shape)\n",
    "print('attention_mask shape:', encoded['attention_mask'].shape)\n",
    "print('input_ids:', encoded['input_ids'][0].tolist())\n",
    "print('attention_mask:', encoded['attention_mask'][0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8252ec",
   "metadata": {},
   "source": [
    "## 8. Token Counting Nuances\n",
    "Raw subword count vs. encoded length (includes special tokens). This matters for API limits & memory planning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8400680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_len = len(tokens)\n",
    "encoded_len = encoded['input_ids'].shape[1]\n",
    "print('Subword token count (no specials):', subword_len)\n",
    "print('Encoded sequence length (with specials):', encoded_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93278305",
   "metadata": {},
   "source": [
    "## 9. Truncation & Padding\n",
    "Controlling length: truncate long inputs, pad shorter to batch uniformly. Attention mask prevents padded positions from influencing outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19f7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = ' '.join(['Transformers']*40)\n",
    "enc_trunc = tokenizer(long_text, max_length=20, truncation=True, padding='max_length', return_tensors='pt')\n",
    "print('Original repeated tokens length (approx raw word count):', len(long_text.split()))\n",
    "print('Truncated+Padded input_ids length:', enc_trunc['input_ids'].shape[1])\n",
    "print('Attention mask:', enc_trunc['attention_mask'][0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f1d7e3",
   "metadata": {},
   "source": [
    "## 10. Batch Tokenization & Simple Length Distribution\n",
    "Batch processing shows variability in token counts; plan for worst-case to avoid overflow. We'll render an ASCII histogram (no plotting dependency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Batch Tokenization & ASCII length distribution\n",
    "ADDITIONAL_SENTENCES = [\n",
    "    'Short prompt.',\n",
    "    'A surprisingly elongated formulation that still conveys a simple idea.',\n",
    "    'Numbers like 123456789012345 can behave oddly.',\n",
    "    'Emojis ðŸ”¥ðŸš€ add tokenization quirks.',\n",
    "    'Hyphenated-terms and URLs https://example.com matter.'\n",
    "]\n",
    "batch_enc = tokenizer(ADDITIONAL_SENTENCES, padding=False)\n",
    "counts = [len(ids) for ids in batch_enc['input_ids']]\n",
    "print('Token counts per sentence:', counts)\n",
    "print('ASCII length bars:')\n",
    "for sent, c in zip(ADDITIONAL_SENTENCES, counts):\n",
    "    bar = '#' * c\n",
    "    print(f'{c:3d} | {bar} | {sent[:40]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfe0568",
   "metadata": {},
   "source": [
    "## 11. Comparing Two Tokenizers\n",
    "Different vocab segmentation strategies change counts â†’ impacts cost & max context usage. Compare WordPiece vs GPT-2 BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d44e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALT_MODEL_NAME = 'gpt2'\n",
    "tok_alt = AutoTokenizer.from_pretrained(ALT_MODEL_NAME)\n",
    "for text in [sample_text] + ADDITIONAL_SENTENCES[:2]:\n",
    "    c_main = len(tokenizer.tokenize(text))\n",
    "    c_alt = len(tok_alt.tokenize(text))\n",
    "    print(f'TEXT: {text[:40]}...')\n",
    "    print(f'  {MODEL_NAME} tokens: {c_main}')\n",
    "    print(f'  {ALT_MODEL_NAME} tokens: {c_alt}')\n",
    "    print(f'  Difference: {c_alt - c_main}')\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58862e",
   "metadata": {},
   "source": [
    "## 12. Estimating Attention Cost\n",
    "Self-attention memory/time ~ O(n^2). Doubling tokens ~4x attention matrix size. Provide a helper for relative cost ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_quadratic_cost(n):\n",
    "    return n * n\n",
    "lengths = [len(t) for t in [tokens, tokenizer.tokenize(long_text)]]\n",
    "small, large = lengths[0], lengths[1]\n",
    "ratio = attention_quadratic_cost(large)/attention_quadratic_cost(small)\n",
    "print(f'Small length: {small}, Large length: {large}')\n",
    "print(f'Relative O(n^2) cost factor â‰ˆ {ratio:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cc97ed",
   "metadata": {},
   "source": [
    "## 13. Practical Utilities\n",
    "Reusable helpers for counting, cost estimation (e.g., API pricing per 1K tokens), and safe truncation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94892b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRICE_PER_1K = 0.003  # example placeholder\n",
    "def count_tokens(text, tok):\n",
    "    return len(tok.tokenize(text))\n",
    "def estimate_cost(num_tokens, price_per_1k=PRICE_PER_1K):\n",
    "    return (num_tokens/1000.0)*price_per_1k\n",
    "def truncate_to_limit(text, tok, limit):\n",
    "    ids = tok(text)['input_ids']\n",
    "    if len(ids) <= limit: return text\n",
    "    # naive: progressively chop words until under limit\n",
    "    words = text.split()\n",
    "    while words and len(tok(' '.join(words))['input_ids']) > limit:\n",
    "        words.pop()\n",
    "    return ' '.join(words)\n",
    "num = count_tokens(sample_text, tokenizer)\n",
    "print('Tokens:', num)\n",
    "print('Estimated cost ($):', round(estimate_cost(num), 6))\n",
    "print('Truncated sample (limit 5 tokens):', truncate_to_limit(long_text, tokenizer, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06dc87b",
   "metadata": {},
   "source": [
    "## 14. Edge Cases\n",
    "Edge forms stress tokenizers: emojis, multilingual text, long numeric strings, URLs. Inspect counts to foresee worst-case consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ed49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_texts = {\n",
    "  'Emojis': 'I love embeddings ðŸ˜„ðŸ”¥ðŸš€',\n",
    "  'Mixed Lang': 'English ä¸Ž ä¸­æ–‡ mixed together.',\n",
    "  'Long Number': 'Transaction ID 123456789012345678901234567890',\n",
    "  'URL': 'Check https://sub.domain.example/path?query=1',\n",
    "  'Code': 'def tokenize(x): return x.split()'\n",
    "}\n",
    "for label, txt in edge_texts.items():\n",
    "    c = count_tokens(txt, tokenizer)\n",
    "    print(f'{label:11s} | tokens={c:3d} | {txt[:50]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432eb4ca",
   "metadata": {},
   "source": [
    "## 15. Mini Exercises\n",
    "1. Find a sentence that produces >2x more word-level tokens than subword tokens.\n",
    "2. Rewrite a verbose prompt to reduce its subword token count by â‰¥20% without losing intent.\n",
    "3. Construct an example where GPT-2 tokenizer yields significantly more tokens than BERT (why?).\n",
    "4. Estimate cost difference between two prompts given a 4K context limit.\n",
    "5. Add a function to cache tokenized results to avoid recomputation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50729ef9",
   "metadata": {},
   "source": [
    "## 16. Summary (Methods vs Trade-offs)\n",
    "| Method | Pros | Cons | Typical Use |\n",
    "|--------|------|------|------------|\n",
    "| Word (split) | Simple | OOV, punctuation issues | Quick baselines |\n",
    "| Char | No OOV | Very long sequences | Specialized scripts, OCR |\n",
    "| Subword (BPE/WordPiece) | Balance length + coverage | Breaks some words into pieces | Modern Transformers |\n",
    "| SentencePiece (Unigram) | Language-agnostic, robust | Slightly slower | Multilingual models |\n",
    "\n",
    "**Key heuristics:** Track token counts early; optimize prompts / inputs by reducing redundancy; choose tokenizer consistent with downstream model ecosystem. Attention cost grows *quadratically* with sequence lengthâ€”every token matters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
