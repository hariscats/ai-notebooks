{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f00f5b",
   "metadata": {},
   "source": [
    "# Mathematical Foundations of Transformer Attention\n",
    "This notebook is a focused, incremental build-up of the math powering Transformer self-attention. Each concept is introduced *only* in the context of why it matters for attention. You alternate between a concise explanation and a small code probe so the abstractions stay grounded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b37ec5",
   "metadata": {},
   "source": [
    "## 1) Core linear algebra primitives (why they matter for attention)\n",
    "- **Vectors**: token embeddings; geometry encodes semantic relationships.\n",
    "  - Similar direction ⇒ similar meaning; length can encode confidence or frequency.\n",
    "- **Dot product**: fast similarity proxy.  $x\\cdot y = \\|x\\|\\,\\|y\\|\\cos\\theta$.\n",
    "  - Used to score how much a query should look at a key.\n",
    "- **Cosine similarity**: normalized dot product removes magnitude: $\\cos\\theta = \\frac{x\\cdot y}{\\|x\\|\\|y\\|}$.\n",
    "- **Matrices**: batch many vectors so we can compare all queries with all keys efficiently.\n",
    "  - $QK^\\top$ forms an $n\\times n$ grid of raw attention logits.\n",
    "- **Softmax**: turns arbitrary real scores into a probability distribution per query row.\n",
    "  - We subtract the row max for numerical stability.\n",
    "  - Masking sets invalid positions to $-\\infty$ so softmax gives them probability 0.\n",
    "- **Scaling**: divide by $\\sqrt{d_k}$ so logits stay in a trainable range as dimensionality grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829fe70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector and dot product demo\n",
    "import numpy as np\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "dot = np.dot(x, y)\n",
    "norm_x = np.linalg.norm(x)\n",
    "norm_y = np.linalg.norm(y)\n",
    "cos_sim = dot / (norm_x * norm_y)\n",
    "print(f'Dot product: {dot}')\n",
    "print(f'Cosine similarity: {cos_sim:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca78e84",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- The dot product measures how aligned two vectors are.\n",
    "- Cosine similarity normalizes by length, so only direction matters.\n",
    "- In Transformers, these operations help compare token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f8cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax normalization demo\n",
    "def softmax(x, temperature=1.0):\n",
    "    x = x / temperature\n",
    "    x = x - np.max(x)  # for numerical stability\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "scores = np.array([2.0, 1.0, 0.1])\n",
    "print('Softmax:', softmax(scores))\n",
    "print('Softmax (high temp):', softmax(scores, temperature=2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeebe40",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Softmax converts scores to probabilities.\n",
    "- Subtracting the max score prevents overflow.\n",
    "- Temperature controls how peaked or flat the distribution is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf3c2b2",
   "metadata": {},
   "source": [
    "## 2) The self-attention pipeline (compact view)\n",
    "1. Linear projections: input embeddings $X \\in \\mathbb{R}^{n\\times d_{model}}$ map to Queries $Q$, Keys $K$, Values $V$.\n",
    "2. Similarity scores: $S = QK^\\top / \\sqrt{d_k}$ (scale controls variance).\n",
    "3. Mask (causal/padding): set disallowed logits to $-\\infty$.\n",
    "4. Normalize: $A = \\mathrm{softmax}(S)$ (row-wise).\n",
    "5. Aggregate: $Z = A V$.\n",
    "6. (Multi-head): split dimensions, repeat independently, then concatenate and mix with $W^O$.\n",
    "Shapes (single head):\n",
    "- $Q,K,V: n\\times d_k$ (or $d_v$ for $V$).\n",
    "- $S: n\\times n$\n",
    "- $A: n\\times n$ (rows sum to 1).\n",
    "- $Z: n\\times d_v$ → projected back to $n\\times d_{model}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2041b9e",
   "metadata": {},
   "source": [
    "# Atomic Math Concepts for Transformer Attention\n",
    "\n",
    "Let's build up from the most basic building blocks, with each concept explained and demonstrated in code.\n",
    "\n",
    "## 1. Scalars\n",
    "A scalar is a single number. Scalars are used for scaling, shifting, and as parameters in formulas.\n",
    "\n",
    "**Example:** $a = 5$\n",
    "\n",
    "**Explanation:**\n",
    "- Scalars are the simplest objects in math. In deep learning, they often represent weights, biases, or single values like temperature in softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44228f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar example\n",
    "scalar = 5\n",
    "print('Scalar:', scalar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bbc12",
   "metadata": {},
   "source": [
    "## 2. Vectors\n",
    "A vector is an ordered list of numbers. Vectors represent points or directions in space.\n",
    "\n",
    "**Example:** $\\mathbf{x} = [1, 2, 3]$\n",
    "\n",
    "**Explanation:**\n",
    "- Vectors are used to represent tokens in NLP models. Each token is mapped to a vector in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector example\n",
    "import numpy as np\n",
    "vector = np.array([1, 2, 3])\n",
    "print('Vector:', vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a588d2",
   "metadata": {},
   "source": [
    "## 3. Dot Product\n",
    "The dot product measures how much two vectors point in the same direction.\n",
    "\n",
    "**Formula:** $\\mathbf{x} \\cdot \\mathbf{y} = \\sum_i x_i y_i$\n",
    "\n",
    "**Example:** $[1, 2, 3] \\cdot [4, 5, 6] = 1\\times4 + 2\\times5 + 3\\times6 = 32$\n",
    "\n",
    "**Explanation:**\n",
    "- The dot product is high when vectors are aligned. In Transformers, it measures similarity between tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb189535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product example\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "dot = np.dot(x, y)\n",
    "print('Dot product:', dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803861e4",
   "metadata": {},
   "source": [
    "## 4. Norm (Length) of a Vector\n",
    "The norm is the length of a vector.\n",
    "\n",
    "**Formula:** $\\|\\mathbf{x}\\| = \\sqrt{\\sum_i x_i^2}$\n",
    "\n",
    "**Example:** $\\|[1, 2, 3]\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14}$\n",
    "\n",
    "**Explanation:**\n",
    "- Norms are used to normalize vectors, which is important for cosine similarity and stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Norm example\n",
    "norm_x = np.linalg.norm(x)\n",
    "print('Norm of x:', norm_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806bf117",
   "metadata": {},
   "source": [
    "## 5. Cosine Similarity\n",
    "Cosine similarity measures the angle between two vectors, ignoring their length.\n",
    "\n",
    "**Formula:** $\\cos(\\theta) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}$\n",
    "\n",
    "**Explanation:**\n",
    "- Cosine similarity is used in attention to compare token meanings, independent of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d16ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity example\n",
    "norm_y = np.linalg.norm(y)\n",
    "cos_sim = dot / (norm_x * norm_y)\n",
    "print('Cosine similarity:', cos_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9febca9",
   "metadata": {},
   "source": [
    "## 6. Matrices\n",
    "A matrix is a grid of numbers. Matrices can represent collections of vectors or transformations.\n",
    "\n",
    "**Example:** $A = \\begin{bmatrix}1 & 2 \\\\ 3 & 4\\end{bmatrix}$\n",
    "\n",
    "**Explanation:**\n",
    "- In Transformers, matrices bundle token vectors and perform batch operations efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8995fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix example\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "print('Matrix A:\\n', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4dc3e4",
   "metadata": {},
   "source": [
    "## 7. Matrix Multiplication\n",
    "Matrix multiplication combines two matrices to produce a new matrix.\n",
    "\n",
    "**Formula:** $(AB)_{ij} = \\sum_k A_{ik} B_{kj}$\n",
    "\n",
    "**Explanation:**\n",
    "- Used to compute all pairwise similarities between queries and keys in attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67721b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication example\n",
    "B = np.array([[2, 0], [1, 2]])\n",
    "product = np.dot(A, B)\n",
    "print('Matrix product AB:\\n', product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9941cb72",
   "metadata": {},
   "source": [
    "## 8. Softmax Function\n",
    "Softmax converts a list of scores into probabilities.\n",
    "\n",
    "**Formula:** $\\mathrm{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$\n",
    "\n",
    "**Explanation:**\n",
    "- Softmax is used in attention to turn similarity scores into weights for mixing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e11e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax example\n",
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "scores = np.array([2.0, 1.0, 0.1])\n",
    "print('Softmax:', softmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3643b4",
   "metadata": {},
   "source": [
    "## 9. From primitives to a minimal attention head\n",
    "Now that we have: dot products (similarity), scaling (stability), softmax (distribution), and matrix multiply (batching), we can assemble a minimal self-attention head for a toy sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fa91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal self-attention head (single head, no masking)\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# Toy sequence: 4 tokens, model dim 6\n",
    "n = 4\n",
    "d_model = 6\n",
    "d_k = d_v = 6\n",
    "X = np.random.randn(n, d_model)\n",
    "\n",
    "# Learned projection matrices (random init for demo)\n",
    "W_Q = np.random.randn(d_model, d_k)\n",
    "W_K = np.random.randn(d_model, d_k)\n",
    "W_V = np.random.randn(d_model, d_v)\n",
    "\n",
    "Q = X @ W_Q  # shape (n, d_k)\n",
    "K = X @ W_K  # shape (n, d_k)\n",
    "V = X @ W_V  # shape (n, d_v)\n",
    "\n",
    "# Similarity scores\n",
    "S = Q @ K.T / np.sqrt(d_k)  # (n, n)\n",
    "\n",
    "# Softmax row-wise\n",
    "S_shift = S - S.max(axis=1, keepdims=True)\n",
    "A = np.exp(S_shift)\n",
    "A /= A.sum(axis=1, keepdims=True)\n",
    "\n",
    "Z = A @ V  # (n, d_v)\n",
    "print('Attention weights (rows sum to 1):')\n",
    "print(np.round(A, 3))\n",
    "print('\\nOutput representations Z:')\n",
    "print(np.round(Z, 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
